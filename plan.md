# Implementation Plan: Notion Workspace Analytics

## ðŸ“‹ Progress Tracker

**Overall Status:** ðŸŸ¡ In Progress
**Last Updated:** 20241003

**Completed Phases:** 0, 1, 2, 3 (+ Unit Tests) | **Current:** Phase 3.5 (Code Quality)
**Code Review:** Completed (see code_review_20241004.md) | **Next:** Phase 4 (Export Extractor)

### Time Estimates
- **Setup & Configuration:** 30 minutes
- **Core Development:** 8-10 hours
- **Testing & Refinement:** 2-3 hours
- **First Run (Data Collection):** 2-10 hours (depends on workspace size)
- **Total:** ~12-15 hours of work + overnight data collection

---

## Phase 0: Project Setup âœ… COMPLETE

**Time:** 30 minutes | **Status:** âœ… Done

- [x] Create project directory structure
- [x] Create `requirements.txt` with dependencies
- [x] Create `.env.example` template
- [x] Create `.gitignore` file
- [x] Create empty Python module files (`src/*.py`)
- [x] Create `README.md` with setup instructions
- [x] Add `.gitkeep` files to data directories

**Files Created:**
- âœ… `requirements.txt` - Python dependencies
- âœ… `.env.example` - Environment template
- âœ… `.gitignore` - Git ignore rules
- âœ… `README.md` - Setup & usage guide
- âœ… `src/__init__.py`, `src/api_client.py`, `src/extractors.py`, `src/analytics.py`, `src/report_builder.py`, `src/utils.py`
- âœ… `data/export/`, `data/cache/`, `data/output/` directories

---

## Phase 1: Environment Setup âœ… COMPLETE

**Time:** 15 minutes | **Status:** âœ… Done

### 1.1 Python Virtual Environment âœ…
- [x] Create virtual environment: `python3 -m venv venv`
- [x] Activate environment: `source venv/bin/activate` (macOS/Linux)
- [x] Upgrade pip: `pip install --upgrade pip`
- [x] Install dependencies: `pip install -r requirements.txt`

### 1.2 Notion API Setup âœ…
- [x] Go to https://www.notion.so/my-integrations
- [x] Create new integration: "Workspace Analytics Bot"
- [x] Set capabilities: Read content + Read user info
- [x] Copy integration token (starts with `secret_` or `ntn_`)
- [x] Add to `.env`: `NOTION_TOKEN=ntn_xxxxx` (or `secret_xxxxx` for older tokens)

### 1.3 Grant Workspace Access âœ…
- [x] Open Notion workspace
- [x] Go to Settings & Members â†’ Connections
- [x] Add integration: "Workspace Analytics Bot"

### 1.4 Export Workspace âœ…
- [x] Settings & Members â†’ Settings â†’ Export all workspace content
- [x] Format: Markdown & CSV
- [x] Include subpages: Yes
- [x] Download and unzip to `./data/export/`

### 1.5 Configure Environment âœ…
- [x] Copy `.env.example` to `.env`: `cp .env.example .env`
- [x] Edit `.env` with Notion token
- [x] Update paths in `.env` (use absolute paths)

**Verification:**
```bash
# Check Python environment
python --version  # Should be 3.9+
pip list | grep notion-client  # Should show notion-client==2.2.1

# Check .env
cat .env | grep NOTION_TOKEN  # Should show your token

# Check export
ls -la data/export/  # Should contain .md files
```

---

## Phase 2: Configuration Module âœ… COMPLETE

**Time:** 15 minutes | **Status:** âœ… Done

### 2.1 Create `config.py` âœ…
- [x] Import dependencies (`os`, `pathlib`, `dotenv`)
- [x] Create `Config` class with constants
- [x] Load environment variables
- [x] Define API configuration (token, rate limits)
- [x] Define directory paths (export, cache, output)
- [x] Define analysis parameters (thresholds)
- [x] Define cost parameters
- [x] Add `validate()` method to check configuration
- [x] Create output directories if they don't exist

**Key Constants:**
```python
NOTION_TOKEN = os.getenv('NOTION_TOKEN')
REQUESTS_PER_SECOND = 3
STALE_THRESHOLD_DAYS = 365
VERY_STALE_THRESHOLD_DAYS = 730
POWER_USER_THRESHOLD = 100
MONTHLY_COST_PER_USER = 12
```

**Test:**
```bash
python -c "from config import Config; Config.validate(); print('âœ“ Config valid')"
```

---

## Phase 3: API Client Module âœ… COMPLETE

**Time:** 1.5 hours | **Status:** âœ… Done

### 3.1 Create `src/api_client.py` âœ…
- [x] Import dependencies (`notion_client`, `time`, `pickle`, `tqdm`)
- [x] Create `NotionAPIClient` class
- [x] Initialize Notion client with token
- [x] Implement rate limiting (`_rate_limit()` method)
- [x] Implement caching helpers:
  - [x] `_get_cache_path(cache_name)` â†’ Path
  - [x] `_load_cache(cache_name)` â†’ Optional[data]
  - [x] `_save_cache(cache_name, data)` â†’ None

### 3.2 Implement API Methods âœ…
- [x] `get_all_users(use_cache=True)` â†’ Dict[user_id, user_info]
  - [x] Fetch users with pagination
  - [x] Cache results
  - [x] Return {user_id: {name, email, type}}

- [x] `search_all_pages(use_cache=True)` â†’ List[Dict]
  - [x] Search all pages with pagination
  - [x] Show progress bar with tqdm
  - [x] Cache results

- [x] `get_page_details(page_id)` â†’ Optional[Dict]
  - [x] Retrieve single page metadata
  - [x] Extract: id, created_time, created_by, last_edited_time, last_edited_by, archived
  - [x] Handle errors gracefully

- [x] `enrich_pages(page_ids, use_cache=True)` â†’ List[Dict]
  - [x] Fetch details for all pages
  - [x] Show progress bar with time estimate
  - [x] Save checkpoint every 1000 pages
  - [x] Cache final results

**Test:**
```bash
python -c "from src.api_client import NotionAPIClient; client = NotionAPIClient(); users = client.get_all_users(); print(f'âœ“ Fetched {len(users)} users')"
```

---

## Phase 3.5: Code Quality Improvements â¬œ TODO

**Time:** 2 hours | **Status:** â¬œ Not Started
**Based on:** Code review recommendations (see code_review_20241004.md)

### 3.5.1 Add API Mocking to Tests (HIGH Priority)
- [ ] Install `pytest-mock`: `pip install pytest-mock`
- [ ] Add to requirements.txt
- [ ] Create mocked tests in `test_api_client.py`:
  - [ ] Mock `get_all_users()` API calls
  - [ ] Mock `search_all_pages()` API calls
  - [ ] Mock `get_page_details()` API calls
  - [ ] Test error scenarios (401, 429, 500)
- [ ] Keep slow integration tests for real API validation
- [ ] Update test documentation

### 3.5.2 Add Type Hints (MEDIUM Priority)
- [ ] Update `config.py` with complete type hints
- [ ] Update `src/api_client.py`:
  - [ ] Fix `Optional[any]` â†’ `Optional[Dict[str, Any]]`
  - [ ] Add return type hints to all methods
  - [ ] Add parameter type hints
- [ ] Install `mypy` for static type checking
- [ ] Add mypy configuration to `pytest.ini` or `pyproject.toml`

### 3.5.3 Create main.py Placeholder (HIGH Priority)
- [ ] Create `main.py` in root directory
- [ ] Add basic structure:
  - [ ] Import all modules
  - [ ] Add `main()` function with orchestration flow
  - [ ] Add `if __name__ == "__main__":` block
  - [ ] Add placeholder comments for Phase 7 implementation
- [ ] Update README.md to reference main.py

### 3.5.4 Add Logging (MEDIUM Priority)
- [ ] Replace `print()` statements with `logging`
- [ ] Configure logging in config.py
- [ ] Update api_client.py to use logger
- [ ] Add log level configuration to .env

### 3.5.5 Documentation Updates
- [ ] Update requirements.txt with new dependencies
- [ ] Document mocking approach in README.md
- [ ] Add type checking instructions
- [ ] Update CLAUDE.md with code quality standards

**Test:**
```bash
pytest tests/ -v  # All mocked tests should pass
mypy src/        # Type checking should pass (after implementation)
python main.py   # Should show orchestration placeholder
```

---

## Phase 4: Export Extractor Module â¬œ TODO

**Time:** 1 hour | **Status:** â¬œ Not Started

### 4.1 Create `src/extractors.py`
- [ ] Import dependencies (`re`, `pathlib`, `config`)
- [ ] Create `ExportExtractor` class

### 4.2 Implement Extraction Methods
- [ ] `extract_page_ids(export_dir=None)` â†’ List[Dict]
  - [ ] Scan all `.md` files recursively
  - [ ] Extract 32-char hex UUID from filename
  - [ ] Format as proper UUID: `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`
  - [ ] Extract page title (filename without UUID)
  - [ ] Get relative path and file size
  - [ ] Return list of {id, title, path, file_size_kb}

- [ ] `detect_databases(export_dir=None)` â†’ List[Dict]
  - [ ] Find folders with matching CSV files
  - [ ] Count entries (`.md` files in folder)
  - [ ] Return list of {name, path, entries}

**Test:**
```bash
python -c "from src.extractors import ExportExtractor; pages = ExportExtractor.extract_page_ids(); print(f'âœ“ Extracted {len(pages)} pages')"
```

---

## Phase 5: Analytics Engine â¬œ TODO

**Time:** 3-4 hours | **Status:** â¬œ Not Started

### 5.1 Create `src/analytics.py`
- [ ] Import dependencies (`pandas`, `numpy`, `datetime`)
- [ ] Create `WorkspaceAnalytics` class
- [ ] Initialize with DataFrame and users dict

### 5.2 Data Preparation
- [ ] `_prepare_dataframe()` - Add derived columns:
  - [ ] Parse `created_time` and `last_edited_time` to datetime
  - [ ] Add `creator_name` and `editor_name` from users dict
  - [ ] Add `created_year`, `created_quarter`, `created_month`
  - [ ] Calculate `days_since_edit`
  - [ ] Add `staleness` category (Active/Fresh/Aging/Stale/Very Stale/Dead)
  - [ ] Detect templates with pattern matching
  - [ ] Flag abandoned pages (`created_time == last_edited_time`)
  - [ ] Flag single-owner pages (`created_by == last_edited_by`)

### 5.3 Implement Analysis Methods

#### 5.3.1 Summary Metrics
- [ ] `_analyze_summary()` â†’ Dict
  - [ ] Total pages, total users, active contributors
  - [ ] Stale percentage, cost per active user

#### 5.3.2 Growth Analysis
- [ ] `_analyze_growth()` â†’ Dict
  - [ ] Annual page counts
  - [ ] Year-over-year growth rates
  - [ ] Quarterly breakdown (latest year)
  - [ ] Monthly breakdown (last 12 months)
  - [ ] Average monthly pages

#### 5.3.3 User Analysis
- [ ] `_analyze_users()` â†’ Dict
  - [ ] User segmentation (power/active/occasional/minimal/non-creators)
  - [ ] Pages created by each segment
  - [ ] Active creator percentage

#### 5.3.4 Top Creators
- [ ] `_analyze_top_creators()` â†’ List[Dict]
  - [ ] Top 10 creators by page count
  - [ ] Percentage of total pages

#### 5.3.5 Content Health
- [ ] `_analyze_content_health()` â†’ Dict
  - [ ] Staleness distribution
  - [ ] Stale (12mo+) and very stale (24mo+) counts
  - [ ] Abandoned pages count
  - [ ] Abandoned by top creators
  - [ ] Archived pages count

#### 5.3.6 Collaboration
- [ ] `_analyze_collaboration()` â†’ Dict
  - [ ] Calculate collaboration scores for each user:
    - [ ] `others_pages = last_edited_by=user AND created_byâ‰ user`
    - [ ] `score = (others_pages / pages_created) Ã— 100`
  - [ ] Top 10 collaborators
  - [ ] Average collaboration score
  - [ ] Collaborated pages count (`created_by â‰  last_edited_by`)
  - [ ] Single-owner pages count

#### 5.3.7 Structure
- [ ] `_analyze_structure()` â†’ Dict
  - [ ] Template count and percentage
  - [ ] Non-template count

#### 5.3.8 Cost Analysis
- [ ] `_analyze_costs()` â†’ Dict
  - [ ] Cost by segment
  - [ ] Total annual cost
  - [ ] Cost per active creator
  - [ ] Wasted spend calculation
  - [ ] ROI calculation (creation value vs. cost)

#### 5.3.9 Risk Assessment
- [ ] `_analyze_risk()` â†’ Dict
  - [ ] Concentration metrics (top 1%, 5%, 10% ownership)
  - [ ] Gini coefficient calculation
  - [ ] Bus factor (people needed for 50% knowledge loss)
  - [ ] Single-owner pages by top 10 creators

### 5.4 Main Orchestration
- [ ] `run_all()` â†’ Dict
  - [ ] Call all analysis methods
  - [ ] Return combined results dict

**Test:**
```bash
python -c "
import pandas as pd
from src.analytics import WorkspaceAnalytics
df = pd.read_csv('data/cache/merged_data.csv')
users = {...}  # Load from cache
analyzer = WorkspaceAnalytics(df, users)
results = analyzer.run_all()
print('âœ“ Analytics complete')
"
```

---

## Phase 6: Report Builder â¬œ TODO

**Time:** 2-3 hours | **Status:** â¬œ Not Started

### 6.1 Create `src/report_builder.py`
- [ ] Import dependencies (`pandas`, `xlsxwriter`, `datetime`)
- [ ] Create `ReportBuilder` class
- [ ] Initialize with results, DataFrame, users

### 6.2 Implement Report Methods

#### 6.2.1 Main Generator
- [ ] `generate_excel(output_path=None)` â†’ Path
  - [ ] Create Excel writer with xlsxwriter engine
  - [ ] Define formats (header, metric, number, percentage, currency)
  - [ ] Call all sheet writers
  - [ ] Return output path

#### 6.2.2 Sheet Writers
- [ ] `_write_executive_summary(writer, header_fmt, metric_fmt)`
  - [ ] Total pages, users, active contributors
  - [ ] Stale content %, annual cost, cost per active user

- [ ] `_write_growth(writer, header_fmt)`
  - [ ] Annual growth table with YoY %
  - [ ] Monthly trend (last 12 months)

- [ ] `_write_users(writer, header_fmt)`
  - [ ] User segmentation table
  - [ ] Top 10 creators table

- [ ] `_write_content_health(writer, header_fmt)`
  - [ ] Staleness distribution
  - [ ] Key metrics (stale, very stale, abandoned, archived)

- [ ] `_write_collaboration(writer, header_fmt)`
  - [ ] Top collaborators table
  - [ ] Summary metrics (avg score, collaborated pages, single-owner pages)

- [ ] `_write_costs(writer, header_fmt)`
  - [ ] Cost by segment table
  - [ ] Summary (total cost, wasted spend, ROI)

- [ ] `_write_risk(writer, header_fmt)`
  - [ ] Concentration risk metrics
  - [ ] Single-owner pages by top 10 creators

- [ ] `_write_raw_data(writer)`
  - [ ] Export full DataFrame with relevant columns

**Test:**
```bash
python -c "
from src.report_builder import ReportBuilder
builder = ReportBuilder(results, df, users)
report_path = builder.generate_excel()
print(f'âœ“ Report generated: {report_path}')
"
```

---

## Phase 7: Main Orchestrator â¬œ TODO

**Time:** 30 minutes | **Status:** â¬œ Not Started

### 7.1 Create `main.py`
- [ ] Import all modules
- [ ] Add main execution flow:

#### 7.1.1 Configuration
- [ ] Print header with timestamp
- [ ] Validate configuration
- [ ] Handle errors gracefully

#### 7.1.2 Data Collection
- [ ] Step 1: Fetch users from API (with cache)
- [ ] Step 2: Extract page IDs from export
- [ ] Step 3: Detect databases from export
- [ ] Step 4: Search pages via API (optional, with cache)
- [ ] Step 5: Enrich pages with metadata (slow, cached)

#### 7.1.3 Data Processing
- [ ] Merge export data + API data into DataFrame
- [ ] Save checkpoint to `data/cache/merged_data.csv`

#### 7.1.4 Analytics
- [ ] Initialize WorkspaceAnalytics
- [ ] Run all analytics
- [ ] Print progress

#### 7.1.5 Report Generation
- [ ] Initialize ReportBuilder
- [ ] Generate Excel report
- [ ] Print output path

#### 7.1.6 Summary
- [ ] Print key findings:
  - [ ] Total pages
  - [ ] Active contributors / total users
  - [ ] Stale content %
  - [ ] Potential savings
  - [ ] Bus factor
- [ ] Print completion timestamp

### 7.2 Entry Point
- [ ] Add `if __name__ == "__main__": main()`

**Test:**
```bash
python main.py
```

---

## Phase 8: Testing & Refinement ðŸŸ¡ PARTIAL

**Time:** 2-3 hours | **Status:** ðŸŸ¡ Partially Complete

### 8.1 Unit Tests âœ…
- [x] Create `tests/test_config.py` (12 tests - all passing)
- [x] Create `tests/test_api_client.py` (11 tests - all passing)
- [x] Create `pytest.ini` configuration
- [x] Add pytest to requirements.txt
- [ ] Create `tests/test_extractors.py` (pending Phase 4)
- [ ] Create `tests/test_analytics.py` (pending Phase 5)
- [ ] Test edge cases (empty data, missing fields)

### 8.2 Error Handling & Retry Logic
- [ ] Add try-except blocks for API calls
- [ ] Handle missing export directory
- [ ] Handle invalid Notion token
- [ ] Handle rate limit errors (429)
- [ ] Add retry logic with exponential backoff:
  - [ ] Install `tenacity` library
  - [ ] Add `@retry` decorator to API methods
  - [ ] Configure: 3 attempts, exponential wait (4-10s)
- [ ] Test error scenarios with mocked responses

### 8.3 Performance Optimization
- [ ] Test with sample workspace
- [ ] Optimize DataFrame operations
- [ ] Reduce memory usage for large workspaces
- [ ] Add batch processing if needed

### 8.4 Documentation
- [ ] Add docstrings to all functions
- [ ] Update README.md with actual usage examples
- [ ] Document common errors and solutions

---

## Phase 9: First Production Run â¬œ TODO

**Time:** 2-10 hours (automated) | **Status:** â¬œ Not Started

### 9.1 Pre-Flight Checklist
- [ ] Virtual environment activated
- [ ] All dependencies installed
- [ ] `.env` configured with valid token
- [ ] Workspace export in `data/export/`
- [ ] Integration connected to workspace

### 9.2 Execute
- [ ] Run: `python main.py`
- [ ] Monitor progress bars
- [ ] Wait for completion (may take hours)

### 9.3 Verify Output
- [ ] Check `data/output/` for Excel report
- [ ] Open Excel file
- [ ] Verify all 8 sheets present
- [ ] Spot-check metrics against known data
- [ ] Review recommendations

### 9.4 Cache Verification
- [ ] Check `data/cache/` for cached files:
  - [ ] `users.pkl`
  - [ ] `enriched_pages.pkl`
  - [ ] `merged_data.csv`
- [ ] Test cached run: `python main.py` (should be <2 min)

---

## Phase 10: Advanced Features (Optional) â¬œ TODO

**Status:** â¬œ Not Started

### 10.1 Utility Scripts
- [ ] `scripts/clear_cache.sh` - Clear cache and force refresh
- [ ] `scripts/anonymize_report.py` - Remove user names from reports
- [ ] `scripts/compare_runs.py` - Compare two analysis runs

### 10.2 Slack Notifications
- [ ] Add Slack webhook to `.env`
- [ ] Create `send_slack_notification()` function
- [ ] Send summary to Slack on completion

### 10.3 Scheduled Execution
- [ ] Create cron job for bi-annual runs
- [ ] Add to crontab: `0 2 1 1,7 * cd /path && python main.py`

### 10.4 Web Dashboard (Future)
- [ ] Install Streamlit: `pip install streamlit`
- [ ] Create `dashboard.py`
- [ ] Add interactive filters
- [ ] Deploy to cloud (Railway/Render)

---

## Quick Reference Commands

### Setup
```bash
# Create environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Configure
cp .env.example .env
# Edit .env with your token
```

### Run Analysis
```bash
# Full run (first time)
python main.py

# Force refresh (clear cache)
rm -rf data/cache/*
python main.py
```

### Check Progress
```bash
# View cached files
ls -lh data/cache/

# View output reports
ls -lh data/output/

# Check Python environment
pip list | grep notion
```

---

## Troubleshooting Checklist

### Issues During Setup
- [ ] Python version â‰¥ 3.9: `python --version`
- [ ] Virtual environment activated: `which python`
- [ ] Dependencies installed: `pip list`
- [ ] `.env` exists and has token: `cat .env | grep NOTION_TOKEN`
- [ ] Export directory has files: `ls data/export/`

### Issues During Execution
- [ ] Check error message in console
- [ ] Verify Notion token is valid
- [ ] Verify integration has workspace access
- [ ] Check internet connection
- [ ] Review rate limiting (reduce REQUESTS_PER_SECOND in config.py)
- [ ] Clear cache if corrupted: `rm -rf data/cache/*`

### Issues with Output
- [ ] Excel file exists: `ls data/output/`
- [ ] File can be opened (not corrupted)
- [ ] All 8 sheets present
- [ ] Data looks reasonable (no nulls or zeros)

---

## Success Criteria

### âœ… Phase Complete When:
- All checkboxes for the phase are checked
- Tests pass (if applicable)
- No errors during execution
- Output matches expected format

### âœ… Project Complete When:
- All phases 0-9 complete
- First production run successful
- Excel report generated with all 8 sheets
- Key metrics match manual spot checks
- Documentation up to date

---

## Notes & Observations

### Implementation Notes:
- Add notes here as you build
- Document decisions made
- Note any deviations from plan

### Performance Metrics:
- First run duration: ___ hours
- Cached run duration: ___ minutes
- Workspace size: ___ pages
- Memory usage: ___ MB

### Issues Encountered:
- Document problems and solutions
- Update troubleshooting section

---

**Next Step:** Complete Phase 1 (Environment Setup)
